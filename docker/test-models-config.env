# Test Configuration for Multi-Model Deployment (AA001)
# Three small models from different organizations for testing

# Disable single model mode by not setting MODEL_ID
# MODEL_ID=

# Multi-Model Configuration
MODELS_CONFIG='[
  {
    "model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    "route_prefix": "/v1",
    "tensor_parallel_size": 1,
    "accelerator_type": null,
    "min_replicas": 1,
    "max_replicas": 1,
    "target_ongoing_requests": 8,
    "max_ongoing_requests": 16,
    "max_num_batched_tokens": 2048,
    "max_model_len": 2048,
    "max_num_seqs": 16
  }
]'

# Common vLLM Engine Configuration
TRUST_REMOTE_CODE=true
ENABLE_PREFIX_CACHING=true

# Route Configuration
ROUTE_PREFIX=/v1

# Port Configuration (defaults)
RAY_API_PORT=8000
RAY_DASHBOARD_PORT=8265
RAY_METRICS_EXPORT_PORT=8080

