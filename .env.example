# Configuration settings for GPU Cluster Platform (vLLM + Ray Serve)
# Copy this file to .env and configure your settings
# All non-sensitive configuration goes here

# ============================================================================
# Ray Serve / vLLM Settings
# ============================================================================

# HuggingFace model identifier
# Example: Qwen/Qwen2.5-32B-Instruct, meta-llama/Llama-2-70b-chat-hf
MODEL_ID=Qwen/Qwen2.5-32B-Instruct

# Optional: Path to local model (for pre-downloaded models)
# MODEL_SOURCE=/path/to/local/model

# Number of GPUs for tensor parallelism (must not exceed GPUs per node)
TENSOR_PARALLEL_SIZE=4

# GPU accelerator type (L4, H100, A100, etc.)
ACCELERATOR_TYPE=L4

# Autoscaling configuration
MIN_REPLICAS=1
MAX_REPLICAS=4
TARGET_ONGOING_REQUESTS=32
MAX_ONGOING_REQUESTS=64
NUM_REPLICAS=1

# vLLM Engine Configuration
MAX_NUM_BATCHED_TOKENS=8192
MAX_MODEL_LEN=8192
MAX_NUM_SEQS=64
TRUST_REMOTE_CODE=true
ENABLE_PREFIX_CACHING=true

# Ray Serve route prefix
ROUTE_PREFIX=/v1

# Ray head node address (for multi-node setups, leave empty for single-node)
# RAY_HEAD_HOST=

# ============================================================================
# Port Configuration
# ============================================================================

# Ray Serve API port (internal)
RAY_API_PORT=8000

# Ray Dashboard port
RAY_DASHBOARD_PORT=8265

# Nginx HTTP port (external)
NGINX_HTTP_PORT=80

# Nginx HTTPS port (external)
NGINX_HTTPS_PORT=443

# ============================================================================
# Nginx Settings
# ============================================================================

# Number of worker processes (auto = detected automatically)
NGINX_WORKER_PROCESSES=auto

# Max request body size
NGINX_CLIENT_MAX_BODY_SIZE=100m
